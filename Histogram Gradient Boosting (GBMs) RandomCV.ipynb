{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8qDInx06aPL"
   },
   "source": [
    "# Histogram Gradient Boosting (GBMs) via Randomized Search CV # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "be_2-2is6aPU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Histogram-based GBMs on TWO datasets: One for the 28x28 images, one for the 64x64 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8RD2kRY6aPV"
   },
   "source": [
    "## 28 x 28 images ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "XO6ykWDc6aPW"
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('data28.csv')\n",
    "y = data['label'].values\n",
    "X = data[data.columns[1:]].values\n",
    "\n",
    "# 70-30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41DwQE-d6aPW",
    "outputId": "1d429135-3a0a-46e4-91de-7b2850c79218",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Randomized search (10 iterations) for 28 x 28 images took 1160.9488430023193 seconds\n"
     ]
    }
   ],
   "source": [
    "grid = {'max_iter' : 250 * np.arange(1, 13)} # 250, 500, ..., 3000\n",
    "gbm=HistGradientBoostingClassifier(max_depth=4, learning_rate=0.01) # choose learning rate 0.01, depth 4\n",
    "gbm_cv_=RandomizedSearchCV(gbm,grid,cv=5,n_jobs=-1,verbose=4,random_state=1,n_iter=5) # run all jobs in parallel, and print the CV accuracy per fold on the terminal (verbose = 4)\n",
    "t0 = time.time()\n",
    "gbm_cv_.fit(X_train,y_train)\n",
    "t1 = time.time()\n",
    "print('Randomized search (5 iterations) for 28 x 28 images took', t1 - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hyperparameters: (best parameters)  {'max_iter': 2750}\n",
      "CV accuracy : 0.9393333333333332\n"
     ]
    }
   ],
   "source": [
    "print(\"tuned hyperparameters: (best parameters) \",gbm_cv_.best_params_)\n",
    "print(\"CV accuracy :\",gbm_cv_.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0R5JNgBP6aPY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 28 x 28 images took 135.37225699424744 seconds\n"
     ]
    }
   ],
   "source": [
    "max_iter = gbm_cv_.best_params_['max_iter']\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=4, max_iter=max_iter, learning_rate=0.01)\n",
    "t0 = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Training for 28 x 28 images took', t1 - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FMa-Qo2Q6aPY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "train_acc = accuracy_score(y_train, clf.predict(X_train)) \n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5qvH4XMx6aPd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611428571428572"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_acc = accuracy_score(y_test, clf.predict(X_test)) \n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cv results (such as mean fit times for each hyperparam configuration) to file\n",
    "# make sure to run the above cells first before running this!!!!\n",
    "with open('gbm_28_randomcv.txt', 'w') as file:\n",
    "    file.write(str(gbm_cv_.cv_results_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO0Bob4c6aPd"
   },
   "source": [
    "## 64 x 64 images ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Pzwzaj4I6aPd"
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('data64.csv')\n",
    "y = data['label'].values\n",
    "X = data[data.columns[1:]].values\n",
    "\n",
    "# 70-30 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.7, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YAMA6ZL36aPe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Randomized search (5 iterations) for 64 x 64 images took 7646.927664041519 seconds\n"
     ]
    }
   ],
   "source": [
    "grid = {'max_iter' : 250 * np.arange(1, 13)} # 250, 500, ..., 3000\n",
    "gbm=HistGradientBoostingClassifier(max_depth=4, learning_rate=0.01) # choose learning rate 0.01, depth 4\n",
    "gbm_cv=RandomizedSearchCV(gbm,grid,cv=5,n_jobs=-1,verbose=4,random_state=1,n_iter=5) # run all jobs in parallel, and print the CV accuracy per fold on the terminal (verbose = 4)\n",
    "t0 = time.time()\n",
    "gbm_cv.fit(X_train,y_train)\n",
    "t1 = time.time()\n",
    "print('Randomized search (5 iterations) for 64 x 64 images took', t1 - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hyperparameters: (best parameters)  {'max_iter': 2750}\n",
      "CV accuracy : 0.9133333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"tuned hyperparameters: (best parameters) \",gbm_cv.best_params_)\n",
    "print(\"CV accuracy :\",gbm_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "P5O-wZRu6aPe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 64 x 64 images took 708.2642109394073 seconds\n"
     ]
    }
   ],
   "source": [
    "max_iter = gbm_cv.best_params_['max_iter']\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=4, max_iter=max_iter, learning_rate=0.01)\n",
    "t0 = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print('Training for 64 x 64 images took', t1 - t0, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RFTlDPLt6aPe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training accuracy\n",
    "train_acc = accuracy_score(y_train, clf.predict(X_train)) \n",
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1BpbprJ26aPe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9388571428571428"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_acc = accuracy_score(y_test, clf.predict(X_test)) \n",
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cv results (such as mean fit times for each hyperparam configuration) to file\n",
    "with open('gbm_64_randomcv.txt', 'w') as file:\n",
    "    file.write(str(gbm_cv.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Histogram Gradient Boosting (GBMs).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
